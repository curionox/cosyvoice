#!/usr/bin/env python3
"""
GPU å’Œ CUDA ç¯å¢ƒè¯Šæ–­è„šæœ¬
ç”¨äºæ’æŸ¥ TensorRT å’Œ CUDA ç›¸å…³é—®é¢˜
"""
import sys
import os

def check_cuda():
    """æ£€æŸ¥ CUDA ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥ CUDA ç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"âœ… GPU æ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
                
                # æµ‹è¯• GPU å†…å­˜åˆ†é…
                try:
                    test_tensor = torch.randn(100, 100).cuda(i)
                    print(f"   âœ… GPU {i} å†…å­˜åˆ†é…æµ‹è¯•æˆåŠŸ")
                    del test_tensor
                    torch.cuda.empty_cache()
                except Exception as e:
                    print(f"   âŒ GPU {i} å†…å­˜åˆ†é…æµ‹è¯•å¤±è´¥: {e}")
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            return False
            
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    
    return True

def check_onnxruntime():
    """æ£€æŸ¥ ONNXRuntime"""
    print("\nğŸ” æ£€æŸ¥ ONNXRuntime...")
    
    try:
        import onnxruntime as ort
        print(f"âœ… ONNXRuntime ç‰ˆæœ¬: {ort.__version__}")
        
        providers = ort.get_available_providers()
        print(f"âœ… å¯ç”¨æä¾›ç¨‹åº: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("âœ… CUDA æ‰§è¡Œæä¾›ç¨‹åºå¯ç”¨")
            
            # æµ‹è¯• CUDA æä¾›ç¨‹åº
            try:
                session = ort.InferenceSession(
                    b'<onnx model placeholder>', 
                    providers=['CUDAExecutionProvider']
                )
                print("âœ… CUDA æä¾›ç¨‹åºæµ‹è¯•æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  CUDA æä¾›ç¨‹åºæµ‹è¯•å¤±è´¥: {e}")
            
            return True
        else:
            print("âŒ CUDA æ‰§è¡Œæä¾›ç¨‹åºä¸å¯ç”¨")
            print("ğŸ’¡ å»ºè®®å®‰è£… onnxruntime-gpu:")
            print("   pip uninstall onnxruntime")
            print("   pip install onnxruntime-gpu")
            return False
            
    except ImportError:
        print("âŒ ONNXRuntime æœªå®‰è£…")
        return False

def check_tensorrt():
    """æ£€æŸ¥ TensorRT"""
    print("\nğŸ” æ£€æŸ¥ TensorRT...")
    
    try:
        import tensorrt as trt
        print(f"âœ… TensorRT ç‰ˆæœ¬: {trt.__version__}")
        
        # æ£€æŸ¥ TensorRT æ„å»ºå™¨
        try:
            logger = trt.Logger(trt.Logger.WARNING)
            builder = trt.Builder(logger)
            print("âœ… TensorRT æ„å»ºå™¨åˆ›å»ºæˆåŠŸ")
            
            # æ£€æŸ¥ GPU è®¾å¤‡
            if builder.max_DLA_batch_size >= 0:
                print("âœ… DLA è®¾å¤‡å¯ç”¨")
            
            print(f"âœ… æœ€å¤§æ‰¹æ¬¡å¤§å°: {builder.max_batch_size}")
            print(f"âœ… æœ€å¤§å·¥ä½œç©ºé—´å¤§å°: {builder.max_workspace_size}")
            
        except Exception as e:
            print(f"âŒ TensorRT æ„å»ºå™¨æµ‹è¯•å¤±è´¥: {e}")
            return False
            
        return True
        
    except ImportError:
        print("âŒ TensorRT æœªå®‰è£…")
        print("ğŸ’¡ å»ºè®®å®‰è£… TensorRT:")
        print("   pip install tensorrt")
        return False

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("\nğŸ” æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯...")
    
    import platform
    print(f"âœ… æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"âœ… Python ç‰ˆæœ¬: {platform.python_version()}")
    
    # æ£€æŸ¥ NVIDIA é©±åŠ¨
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version' in line:
                    print(f"âœ… NVIDIA é©±åŠ¨: {line.split('Driver Version: ')[1].split()[0]}")
                    break
        else:
            print("âŒ nvidia-smi å‘½ä»¤å¤±è´¥")
    except Exception as e:
        print(f"âŒ æ— æ³•æ£€æŸ¥ NVIDIA é©±åŠ¨: {e}")

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ” æµ‹è¯• CosyVoice æ¨¡å‹åŠ è½½...")
    
    # æ·»åŠ è·¯å¾„
    sys.path.append('third_party/Matcha-TTS')
    
    try:
        from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
        
        model_dir = 'pretrained_models/CosyVoice-300M-SFT'
        if not os.path.exists(model_dir):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_dir}")
            return False
        
        print("ğŸ”„ æµ‹è¯•åŸºç¡€æ¨¡å‹åŠ è½½...")
        try:
            cosyvoice = CosyVoice(model_dir)
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
            del cosyvoice
        except Exception as e:
            print(f"âŒ åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        print("ğŸ”„ æµ‹è¯• JIT æ¨¡å‹åŠ è½½...")
        try:
            cosyvoice = CosyVoice(model_dir, load_jit=True)
            print("âœ… JIT æ¨¡å‹åŠ è½½æˆåŠŸ")
            del cosyvoice
        except Exception as e:
            print(f"âŒ JIT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        print("ğŸ”„ æµ‹è¯• FP16 æ¨¡å‹åŠ è½½...")
        try:
            cosyvoice = CosyVoice(model_dir, fp16=True)
            print("âœ… FP16 æ¨¡å‹åŠ è½½æˆåŠŸ")
            del cosyvoice
        except Exception as e:
            print(f"âŒ FP16 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ CosyVoice å¯¼å…¥å¤±è´¥: {e}")
        return False

def main():
    print("=" * 60)
    print("CosyVoice GPU ç¯å¢ƒè¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    results = {
        'cuda': check_cuda(),
        'onnxruntime': check_onnxruntime(),
        'tensorrt': check_tensorrt(),
        'model': False
    }
    
    check_system_info()
    
    if results['cuda']:
        results['model'] = test_model_loading()
    
    print("\n" + "=" * 60)
    print("è¯Šæ–­ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    print(f"CUDA ç¯å¢ƒ: {'âœ… æ­£å¸¸' if results['cuda'] else 'âŒ å¼‚å¸¸'}")
    print(f"ONNXRuntime: {'âœ… æ­£å¸¸' if results['onnxruntime'] else 'âŒ å¼‚å¸¸'}")
    print(f"TensorRT: {'âœ… æ­£å¸¸' if results['tensorrt'] else 'âŒ å¼‚å¸¸'}")
    print(f"æ¨¡å‹åŠ è½½: {'âœ… æ­£å¸¸' if results['model'] else 'âŒ å¼‚å¸¸'}")
    
    print("\nğŸ’¡ å»ºè®®çš„å¯åŠ¨é…ç½®:")
    
    if all(results.values()):
        print("ğŸš€ æ¨èä½¿ç”¨å®Œæ•´ä¼˜åŒ–é…ç½®:")
        print("python fastapi_server.py --model_dir pretrained_models/CosyVoice-300M-SFT --port 9234 --load_trt --load_jit --fp16")
    elif results['cuda'] and results['onnxruntime']:
        print("âš¡ æ¨èä½¿ç”¨ä¸­ç­‰ä¼˜åŒ–é…ç½®:")
        print("python fastapi_server.py --model_dir pretrained_models/CosyVoice-300M-SFT --port 9234 --load_jit --fp16")
    elif results['cuda']:
        print("âœ… æ¨èä½¿ç”¨åŸºç¡€ä¼˜åŒ–é…ç½®:")
        print("python fastapi_server.py --model_dir pretrained_models/CosyVoice-300M-SFT --port 9234 --load_jit")
    else:
        print("ğŸŒ æ¨èä½¿ç”¨ CPU é…ç½®:")
        print("python fastapi_server.py --model_dir pretrained_models/CosyVoice-300M-SFT --port 9234")
    
    print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
    if not results['cuda']:
        print("- æ£€æŸ¥ CUDA å®‰è£…å’Œ GPU é©±åŠ¨")
        print("- é‡æ–°å®‰è£… PyTorch GPU ç‰ˆæœ¬")
    if not results['onnxruntime']:
        print("- å®‰è£… onnxruntime-gpu æ›¿æ¢ onnxruntime")
    if not results['tensorrt']:
        print("- å®‰è£… TensorRT æˆ–ä½¿ç”¨é TRT é…ç½®")
        print("- æ£€æŸ¥ TensorRT ä¸ CUDA ç‰ˆæœ¬å…¼å®¹æ€§")

if __name__ == '__main__':
    main()
